# From Model-Level Explanations to Legal Evidence
## Auditing Bias and Explainability in GDPR-Compliant Automated Decision Systems

**SSRN:** [Abstract page](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263) · **PDF:** [ssrn-5783263.pdf](publications/ssrn-5783263.pdf) · **Figures:** [visuals/](visuals/) · **Evidence pack:** [publications/](publications/)
## Start with the PDF (the full AI governance toolkit)

**From Model-Level Explanations to Legal Evidence: Auditing Bias and Explainability in GDPR-Compliant Automated Decision Systems**

 **Open the full paper (PDF):** `publications/ssrn-5783263.pdf`  
 **SSRN abstract page:** https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263  

If you work in **AI governance, Model Risk Management, Compliance, Risk & Controls, Data Protection, Supplier Assurance, or Responsible AI**, the README visuals are only the preview.

The **PDF is the full evidence playbook**: it shows how to take *model outputs* (explainability + fairness results) and turn them into **audit-ready, governance-ready documentation** that supports review, challenge, and accountability in real operational settings.

### Why this PDF matters for day-to-day governance
Because “we have SHAP” is not the same as “we can defend this decision process.”

In practice, governance teams need:
- a clear explanation of **what drives outcomes** at model level,
- a way to spot and communicate **potential disparities** across groups,
- language and structure that can survive **internal scrutiny, audit, and regulatory questioning**,
- a repeatable approach to documenting risk and safeguards where automated decisions have significant impact.

This paper is written to help you do exactly that.

### What you get when you open the PDF (usable takeaways)
- **A repeatable audit logic**: how to structure your evidence trail so it’s reviewable and defensible.
- **Model-level explainability** (SHAP global): how to interpret key drivers and communicate them clearly.
- **Fairness testing evidence** (sex + age group): how to read selection-rate patterns and flag potential disparity signals.
- **Interpretation that leads to action**: what to do next (monitoring, thresholds, escalation, documentation updates) when signals appear.
- **Governance framing**: how to connect technical results to accountability, transparency, and meaningful oversight in practice.

### Recommended reading path (if you’re busy)
- **2 minutes:** open PDF → scan headings + figures to understand the flow.
- **10 minutes:** read the explainability + fairness interpretation sections.
- **30 minutes:** follow the full evidence chain and reuse the structure for your own model review pack.

### Bottom line
If you want the *real value* — the logic, the interpretation, and the evidence structure you can reuse at work — **open the PDF**.
The README shows the highlights; the paper gives you the full governance method.


### Recruiter summary (30 seconds)
This repository is a **Responsible AI audit evidence pack** showing how to translate:
- **Explainability** (SHAP = global patterns + case-level explanations)
- **Fairness testing** (group metrics + disparity interpretation)
into **GDPR-ready documentation** for automated decision systems (Article 22-relevant contexts).

## What you can open immediately
- **Working paper (SSRN):** https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263
- **Key visuals:** `visuals/` (SHAP + fairness outputs)
- **Evidence pack / publication files:** `publications/`
- **GitHub Pages preview:** https://22ifeoma22.github.io/responsible-ai-portfolio-preview/

---

## Key figures (preview)

![SHAP global explanation (beeswarm)](visuals/SHAP%20global%20explanation%20(beeswarm).png)
![Fairness — Selection rate by sex (male vs female)](visuals/Fairness%20-%20Selection%20rate%20by%20sex%20(male%20vs%20female).png)
![Fairness — Selection rate by age group](visuals/Fairness%20-%20Selection%20rate%20by%20age%20group.png)

---

## Evidence → governance question mapping
| Evidence artifact | What it shows | Governance question answered |
|---|---|---|
| SHAP (global) | Main drivers across population | “Can we explain outcomes at model level?” |
| LIME (local) | Why one person got an outcome | “Can we support review/challenge for individuals?” |
| Fairness metrics | Group disparities | “Are outcomes disproportionately adverse?” |
| Audit narrative | limits + safeguards | “What controls reduce risk and enable rights?” |

---

## Working paper (SSRN)
**Title:** *From Model-Level Explanations to Legal Evidence: Auditing Bias and Explainability in GDPR-Compliant Automated Decision Systems*  
Read on SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263
