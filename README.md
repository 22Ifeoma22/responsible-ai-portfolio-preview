# From Model-Level Explanations to Legal Evidence
## Auditing Bias and Explainability in GDPR-Compliant Automated Decision Systems

**SSRN:** [Abstract page](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263) · **PDF:** [paper.pdf](publications/<paper-folder>/paper.pdf) · **Figures:** [visuals/](visuals/) · **Evidence pack:** [publications/](publications/)

### Recruiter summary (30 seconds)
This repository is a **Responsible AI audit evidence pack** showing how to translate:
- **Explainability** (SHAP = global patterns, LIME = individual decisions)
- **Fairness testing** (group metrics + disparity interpretation)
into **GDPR-ready documentation** for automated decision systems (Article 22-relevant contexts).

## What you can open immediately
- **Working paper (SSRN):** https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263
- **Key visuals:** `visuals/` (SHAP, LIME, fairness outputs)
- **Evidence pack / publication files:** `publications/`
- **GitHub Pages preview:** (your Pages link)

---

## Key figures (preview)
> Replace the filenames below with your real image names in `/visuals`.

![SHAP global](visuals/<shap_image>.png)
![LIME local](visuals/<lime_image>.png)
![Fairness](visuals/<fairness_image>.png)

---

## Evidence → governance question mapping
| Evidence artifact | What it shows | Governance question answered |
|---|---|---|
| SHAP (global) | Main drivers across population | “Can we explain outcomes at model level?” |
| LIME (local) | Why one person got an outcome | “Can we support review/challenge for individuals?” |
| Fairness metrics | Group disparities | “Are outcomes disproportionately adverse?” |
| Audit narrative | limits + safeguards | “What controls reduce risk and enable rights?” |

---

## Working paper (SSRN)
**Title:** *From Model-Level Explanations to Legal Evidence: Auditing Bias and Explainability in GDPR-Compliant Automated Decision Systems*  
Read on SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5783263
